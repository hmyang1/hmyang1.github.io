---
---

@string{cvpr = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)}}
@string{iccv = {{IEEE/CVF} International Conference on Computer Vision (<b>ICCV</b>)}}
@string{iccvw = {{IEEE/CVF} International Conference on Computer Vision Workshop (<b>ICCVW</b>)}}
@string{eccv = {European Conference on Computer Vision (<b>ECCV</b>)}}
@string{neurips = {Conference on Neural Information Processing Systems (<b>NeurIPS</b>)}}
@string{aaai = {Association for the Advancement of Artificial Intelligence (<b>AAAI</b>)}}

@string{gtc = {NVIDIA GPU Technology Conference (<b>GTC</b>)}}
@string{iccas = {International Conference on Control, Automation and Systems (<b>ICCAS</b>)}}


# Selected Papers
@inproceedings{Yang_2024_AAAI,
  author    = {Hunmin Yang* and Jongoh Jeong* and Kuk-Jin Yoon},
  title     = {FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks},
  abstract  = {Deep neural networks are known to be vulnerable to security risks due to the inherent transferable nature of adversarial examples. Despite the success of recent generative model-based attacks demonstrating strong transferability, it still remains a challenge to design an efficient attack strategy in a real-world strict black-box setting, where both the target domain and model architectures are unknown. In this paper, we seek to explore a feature contrastive approach in the frequency domain to generate adversarial examples that are robust in both cross-domain and cross-model settings. With that goal in mind, we propose two modules that are only employed during the training phase: a Frequency-Aware Domain Randomization (FADR) module to randomize domain-variant low- and high-range frequency components and a Frequency-Augmented Contrastive Learning (FACL) module to effectively separate domain-invariant mid-frequency features of clean and perturbed image. We demonstrate strong transferability of our generated adversarial perturbations through extensive cross-domain and cross-model experiments, while keeping the inference time complexity.},
  booktitle = aaai,
  year      = {2024},
  abbr={AAAI},
  selected={true},
  website={https://aaai.org/aaai-conference/},
  additional_info2={ICCV Workshop on Adversarial Robustness In the Real World (AROW) 2023},
  img_path={assets/img/facl.png},
  equal_contrib={true}
}

@InProceedings{Cho_2023_ICCV,
  author    = {Junhyeong Cho and Gilhyun Nam and Sungyeon Kim and Hunmin Yang and Suha Kwak},
  abstract  = {In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Also, a recent study has demonstrated the cross-modal transferability phenomenon of this joint space. From these observations, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. The proposed method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, even though it does not require any images for training.},
  title     = {PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization},
  booktitle = iccv,
  year      = {2023},
  abbr={ICCV},
  arxiv={2307.15199},
  selected={true},
  website={https://promptstyler.github.io/},
  img_path={assets/img/promptstyler.png},
}

@InProceedings{Suryanto_2023_ICCV,
  author    = {Naufal Suryanto and Yongsu Kim and Harashta Tatimma Larasati and Hyoeun Kang and Thi-Thu-Huong Le and Yoonyoung Hong and Hunmin Yang and Se-Yoon Oh and Howon Kim},
  abstract  = {Adversarial camouflage has garnered attention for its ability to attack object detectors from any viewpoint by covering the entire object's surface. However, universality and robustness in existing methods often fall short as the transferability aspect is often overlooked, thus restricting their application only to a specific target with limited performance. To address these challenges, we present Adversarial Camouflage for Transferable and Intensive Vehicle Evasion (ACTIVE), a state-of-the-art physical camouflage attack framework designed to generate universal and robust adversarial camouflage capable of concealing any 3D vehicle from detectors. Our framework incorporates innovative techniques to enhance universality and robustness, including a refined texture rendering that enables common texture application to different vehicles without being constrained to a specific texture map, a novel stealth loss that renders the vehicle undetectable, and a smooth and camouflage loss to enhance the naturalness of the adversarial camouflage. Our extensive experiments on 15 different models show that ACTIVE consistently outperforms existing works on various public detectors, including the latest YOLOv7. Notably, our universality evaluations reveal promising transferability to other vehicle classes, tasks (segmentation models), and the real world, not just other vehicles.},
  title     = {ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion},
  booktitle = iccv,
  year      = {2023},
  abbr={ICCV},
  arxiv={2308.07009},
  selected={true},
  website={https://islab-ai.github.io/active-iccv2023/},
  img_path={assets/img/active.jpg},
}

@InProceedings{Suryanto_2022_CVPR,
  author    = {Naufal Suryanto and Yongsu Kim and Hyoeun Kang and Harashta Tatimma Larasati and Youngyeo Yun and Thi-Thu-Huong Le and Hunmin Yang and Se-Yoon Oh and Howon Kim},
  abstract  = {To perform adversarial attacks in the physical world, many studies have proposed adversarial camouflage, a method to hide a target object by applying camouflage patterns on 3D object surfaces. For obtaining optimal physical adversarial camouflage, previous studies have utilized the so-called neural renderer, as it supports differentiability. However, existing neural renderers cannot fully represent various real-world transformations due to a lack of control of scene parameters compared to the legacy photo-realistic renderers. In this paper, we propose the Differentiable Transformation Attack (DTA), a framework for generating a robust physical adversarial pattern on a target object to camouflage it against object detection models with a wide range of transformations. It utilizes our novel Differentiable Transformation Network (DTN), which learns the expected transformation of a rendered object when the texture is changed while preserving the original properties of the target object. Using our attack framework, an adversary can gain both the advantages of the legacy photo-realistic renderers including various physical-world transformations and the benefit of white-box access by offering differentiability. Our experiments show that our camouflaged 3D vehicles can successfully evade state-of-the-art object detection models in the photo-realistic environment (i.e., CARLA on Unreal Engine). Furthermore, our demonstration on a scaled Tesla Model 3 proves the applicability and transferability of our method to the real world.},
  title     = {DTA: Physical Camouflage Attacks Using Differentiable Transformation Network},
  booktitle = cvpr,
  year      = {2022},
  abbr={CVPR},
  arxiv={2203.09831},
  selected={true},
  website={https://islab-ai.github.io/dta-cvpr2022/},
  img_path={assets/img/dta.png},
}



# GTC
@inproceedings{Yang_2023_GTC,
  author    = {Hunmin Yang and Se-Yoon Oh and Junhyeong Cho},
  title     = {Synthetic Image Generation for Deep Neural Networks},
  booktitle = gtc,
  year      = {2023},
  abbr={GTC},
  selected={false},
  website={https://www.nvidia.com/en-us/on-demand/session/gtcspring23-ps51191/},
  additional_info={Spotlight Presentation},
  img_path={assets/img/gtc2023.jpg},
}

@inproceedings{Yang_2020_GTC,
  author    = {Hunmin Yang and Se-Yoon Oh and Taewon Kim and Ki-Jung Ryu},
  title     = {D-GEN: A Deep Learning Data Generation Framework for Artificial Intelligence},
  booktitle = gtc,
  year      = {2020},
  abbr={GTC},
  selected={false},
  website={https://www.nvidia.com/gtc/poster-gallery/},
  img_path={assets/img/gtc2020.jpg},
}

@inproceedings{Yang_2019_GTC,
  author    = {Hunmin Yang and Se-Yoon Oh and Ki-Jung Ryu},
  title     = {Accelerating Distributed Deep Learning Inference on multi-GPU with Hadoop-Spark},
  booktitle = gtc,
  year      = {2019},
  abbr={GTC},
  selected={false},
  website={https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2019-s9343/},
  additional_info={Oral Talk (50min)},
  img_path={assets/img/gtc2019.jpg},
}



# ICCAS
@inproceedings{Kim_2021_ICCAS,
  author    = {Jeonghun Kim and Kyungmin Lee and Hyeongkeun Lee and Hunmin Yang and Se-Yoon Oh},
  title     = {Camouflaged Adversarial Attack on Object Detector},
  booktitle = iccas,
  year      = {2021},
  abbr={ICCAS},
  selected={false},
  website={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650004},
  img_path={assets/img/iccas2021.png},
}
